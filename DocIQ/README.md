# ‚¨° DocIQ ‚Äî Advanced Document Intelligence System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-1.35+-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![FAISS](https://img.shields.io/badge/FAISS-Vector_DB-0467DF?style=for-the-badge&logo=meta&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-000000?style=for-the-badge&logo=ollama&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**Upload any document. Ask anything. Get answers grounded in your content.**

[Features](#-features) ‚Ä¢ [Architecture](#-architecture) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Deployment](#-deployment) ‚Ä¢ [Models](#-models-used)

</div>

---

## üìå What is DocIQ?

DocIQ is a **production-grade Retrieval-Augmented Generation (RAG) system** that transforms static documents into an interactive conversational intelligence layer. Upload a PDF, DOCX, or TXT file and instantly chat with it, summarize it, extract structured data, or search it semantically ‚Äî all with answers strictly grounded in your document content.

> **No hallucination. No guessing. Every answer comes from your document.**

---

## ‚ú® Features

| Feature | Description |
|---|---|
| üí¨ **Conversational QA** | Chat with your document like a conversation with full memory |
| üìã **Multi-Mode Summarization** | TL;DR, Executive, Technical, Bullet Points, Hierarchical |
| üîç **Structured Extraction** | Auto-extract entities, statistics, conclusions as JSON |
| üß≠ **Semantic Search** | Find relevant sections by meaning, not keywords |
| üìä **Evaluation Metrics** | ROUGE scores, latency tracking, retrieval confidence |
| üîí **Anti-Hallucination** | LLM strictly restricted to retrieved document context |
| üß† **Conversation Memory** | Sliding window + LLM-compressed memory across turns |
| üìé **Multi-Format Support** | PDF, DOCX, TXT, Markdown |
| ü¶ô **100% Local Option** | Run with Ollama ‚Äî no API key, no internet, no cost |

---

## üèó Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DocIQ Pipeline                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  DOCUMENT                                                ‚îÇ
‚îÇ  UPLOAD    ‚Üí  Parser  ‚Üí  Chunker  ‚Üí  Embedder  ‚Üí  FAISS ‚îÇ
‚îÇ                                           ‚Üë              ‚îÇ
‚îÇ                                    MiniLM-L6-v2          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  USER                                                    ‚îÇ
‚îÇ  QUESTION  ‚Üí  Embed Query  ‚Üí  FAISS Search  ‚Üí  Top-K    ‚îÇ
‚îÇ                                                   ‚Üì      ‚îÇ
‚îÇ                              Memory  ‚Üí  Prompt Builder   ‚îÇ
‚îÇ                                                   ‚Üì      ‚îÇ
‚îÇ                                         LLM (Mistral /   ‚îÇ
‚îÇ                                         Claude / GPT)    ‚îÇ
‚îÇ                                                   ‚Üì      ‚îÇ
‚îÇ                                            ANSWER ‚úÖ     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Components

```
dociq/
‚îú‚îÄ‚îÄ app.py                      # Streamlit UI ‚Äî chat, summarize, extract, search
‚îú‚îÄ‚îÄ config.py                   # Typed configuration for all system parameters
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py   # PDF / DOCX / TXT parsing with section detection
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py              # Token-aware + section-aware chunking (512 tok, 64 overlap)
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py           # Sentence-transformers embedding engine
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py         # FAISS index + MMR retrieval + confidence scoring
‚îÇ   ‚îú‚îÄ‚îÄ llm_engine.py           # Multi-provider LLM (Anthropic / OpenAI / Ollama / Groq)
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py         # End-to-end RAG orchestrator + conversation memory
‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py            # ROUGE, latency, retrieval quality metrics
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ templates.py            # Prompt library for QA, summarization, extraction
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ session.py              # Streamlit session state management
```

---

## üöÄ Installation

### Prerequisites
- Python 3.11+
- 8GB RAM minimum (16GB recommended)
- [Ollama](https://ollama.com) (for local free usage)

### Step 1 ‚Äî Clone the repository
```bash
git clone https://github.com/YOURNAME/dociq.git
cd dociq
```

### Step 2 ‚Äî Create virtual environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Mac / Linux
source venv/bin/activate
```

### Step 3 ‚Äî Install dependencies
```bash
pip install -r requirements.txt
```

### Step 4 ‚Äî Set up LLM backend

**Option A ‚Äî Ollama (Free, Local, Recommended)**
```bash
# Install Ollama from https://ollama.com
ollama pull mistral:7b        # or llama3.2:3b for faster/lighter

# Start Ollama server
ollama serve
```

**Option B ‚Äî Anthropic Claude**
```bash
# Create .env file
echo ANTHROPIC_API_KEY=sk-ant-your-key-here > .env
```

**Option C ‚Äî Groq (Free API)**
```bash
# Sign up free at console.groq.com
echo GROQ_API_KEY=your-key-here > .env
```

### Step 5 ‚Äî Launch
```bash
streamlit run app.py
```

Open `http://localhost:8501` in your browser.

---

## üñ• Usage

### Basic Workflow
1. **Select LLM backend** in the sidebar (Ollama / Anthropic / OpenAI)
2. **Upload a document** (PDF, DOCX, or TXT)
3. Click **"‚ö° Build Index & Start"** ‚Äî embeds and indexes your document
4. **Start chatting** in the Chat tab

### Chat Tab
Ask anything about your document:
```
"What is the main conclusion of this paper?"
"What methodology was used?"
"Summarize the results section"
"Who are the authors?"
```

### Summarize Tab
Choose a mode:
- **TL;DR** ‚Äî 2-3 sentence essence
- **Executive** ‚Äî Business-oriented with key findings
- **Technical** ‚Äî Preserves methodology and metrics
- **Bullet Points** ‚Äî Scannable key points
- **Hierarchical** ‚Äî Section-by-section then global synthesis

### Extract Tab
Auto-extracts structured JSON:
```json
{
  "title": "...",
  "authors": ["..."],
  "named_entities": {
    "people": ["..."],
    "organizations": ["..."],
    "locations": ["..."]
  },
  "key_statistics": [...],
  "main_conclusions": [...]
}
```

### Search Tab
Semantic search across all document chunks ‚Äî finds by meaning not keywords.

### Metrics Tab
Real-time performance dashboard:
- Retrieval latency, generation latency, P95 latency
- Top cosine similarity, average confidence
- ROUGE-1, ROUGE-2, ROUGE-L scores

---

## ü§ñ Models Used

| Model | Role | Size | Provider |
|---|---|---|---|
| `all-MiniLM-L6-v2` | Text ‚Üí Embeddings (384-dim) | 80MB | Microsoft / HuggingFace |
| `mistral:7b` | LLM ‚Äî Answer generation | 4.1GB | Mistral AI via Ollama |
| `llama3.1:8b` | LLM ‚Äî Alternative | 4.7GB | Meta via Ollama |
| `claude-sonnet-4` | LLM ‚Äî Cloud option | API | Anthropic |
| `FAISS IndexFlatIP` | Vector similarity search | ‚Äî | Meta |

---

## ‚öôÔ∏è Configuration

All settings in `config.py`:

```python
# Chunking
chunk_size = 512          # tokens per chunk
chunk_overlap = 64        # overlap between chunks

# Retrieval
top_k = 5                 # chunks to retrieve
similarity_threshold = 0.20
use_mmr = True            # Maximal Marginal Relevance
mmr_lambda = 0.7          # relevance vs diversity balance

# Memory
memory_window = 10        # last N conversation turns to keep
```

---

## üåê Deployment

### Streamlit Cloud (Free, Recommended)
1. Push code to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io)
3. Connect your repo ‚Üí set `app.py` as main file
4. Add secret: `ANTHROPIC_API_KEY = "sk-ant-..."`
5. Deploy ‚Üí get public URL

### Groq + Streamlit Cloud (Completely Free)
1. Get free API key at [console.groq.com](https://console.groq.com)
2. Deploy on Streamlit Cloud with `GROQ_API_KEY` secret
3. Zero cost, public URL, production ready

### Oracle Cloud Free VM (Always On, No API Key)
```bash
# On Oracle free VM (4 CPU, 24GB RAM)
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
ollama pull llama3.2:3b
git clone https://github.com/YOURNAME/dociq.git
cd dociq && pip install -r requirements.txt
streamlit run app.py --server.port=8501 --server.address=0.0.0.0
```

### Quick Demo (ngrok)
```bash
# Run app locally then expose publicly
streamlit run app.py
ngrok http 8501
# Share the ngrok URL
```

---

## üî¨ Technical Details

### Chunking Strategy
- **Phase 1:** Split by document sections (headings, numbered sections) ‚Äî never breaks logical units
- **Phase 2:** Token-aware sliding window within each section (512 tokens, 64 overlap)
- **Phase 3:** Metadata enrichment ‚Äî section title prepended to chunk for better embedding quality

### Retrieval Strategy
```
FAISS k-NN (k = top_k √ó 3)
    ‚Üí Similarity threshold filter
    ‚Üí MMR reranking for diversity
    ‚Üí Token budget management (5000 token max context)
    ‚Üí Confidence score normalization
```

### MMR Formula
```
MMR(d) = Œª √ó sim(query, d) ‚àí (1‚àíŒª) √ó max{sim(d, s) : s ‚àà Selected}
```

### Anti-Hallucination
Every prompt contains:
```
Answer EXCLUSIVELY from the provided document context.
Do NOT use external knowledge.
If the answer is not found, say: "I cannot find this in the document."
```

### Conversation Memory
```
Recent turns (last 10) ‚Üí verbatim in prompt
Older turns (>20)      ‚Üí LLM-compressed summary
```

---

## üìä Evaluation Metrics

| Metric | Description | Target |
|---|---|---|
| ROUGE-1 F1 | Unigram overlap with reference | > 0.40 |
| ROUGE-2 F1 | Bigram overlap | > 0.20 |
| ROUGE-L F1 | Longest common subsequence | > 0.35 |
| Retrieval Latency | FAISS search time | < 50ms |
| Generation Latency | LLM response time | < 3000ms |
| Top Similarity | Best chunk cosine score | > 0.55 |

---

## üõ° Security Notes

- Never commit `.env` files or API keys to GitHub
- Add `.env` and `*.log` to `.gitignore`
- For sensitive documents use local Ollama ‚Äî data never leaves your machine
- API keys shown in sidebar are stored in session state only ‚Äî not persisted to disk

---

## üîÆ Roadmap

- [ ] Cross-encoder reranker (BGE-reranker-large)
- [ ] OCR support for scanned PDFs (Tesseract)
- [ ] Multi-document indexing with source filtering
- [ ] BERTScore evaluation
- [ ] FastAPI backend for production
- [ ] Pinecone / Qdrant cloud vector DB support
- [ ] Export chat history as PDF

---

## üìÑ License

MIT License ‚Äî free to use, modify, and distribute.

---

## üôè Built With

- [Streamlit](https://streamlit.io) ‚Äî Web UI framework
- [Sentence Transformers](https://sbert.net) ‚Äî Embedding models
- [FAISS](https://faiss.ai) ‚Äî Vector similarity search
- [Ollama](https://ollama.com) ‚Äî Local LLM runner
- [pdfplumber](https://github.com/jsvine/pdfplumber) ‚Äî PDF extraction
- [Anthropic](https://anthropic.com) ‚Äî Claude API

---

## Author
- *Aadil Hussain*

<div align="center">
Built with ‚ù§Ô∏è as a portfolio project demonstrating production RAG architecture
</div>
